<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Sastibe&#39;s Data Science Blog</title>
    <link>http://www.sastibe.de/post/</link>
    <description>Recent content in Posts on Sastibe&#39;s Data Science Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jul 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.sastibe.de/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Registering my Blog at the VG Wort</title>
      <link>http://www.sastibe.de/2018/07/registering-my-blog-at-the-vg-wort/</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/07/registering-my-blog-at-the-vg-wort/</guid>
      <description>Suddenly, Attention So, it seems like my blog is getting some attention. Recently, I was even featured on the front page of Hacker News:
Number 17... only 16 more to go!

Obviously, I am delighted and flattered by the number of people reading and discussing my blog. But since we&#39;re living in a material world, and I am a material guy, I kept on wondering &#34;is there any way to monetize this in an ethical way?&#34;. Obviously, advertisements are out of the question, but are there other ways?
VG Wort The VG Wort, or Verwertungsgesellschaft Wort is a German association tasked with collecting and distributing profits made secondary exploitation rights of original textual contents. Essentially, it tries to reimburse authors and journalists for the money they &#34;lose&#34; by texts that are copied or digitally reproduced and thus &#34;free of charge&#34;. The money received by the VG Wort stems mostly from so ca lled Kopierabgabegeräten 1.
A Step-by-Step Guide for registering your Blog at VG Wort In what follows, I recount the steps I needed to take to register my first blog post in VG Wort. Note that all of these steps are exclusively available to German citizens, which is unfortunate: An internanationally available solution like the one provided by VG Wort for &#34;making money on the internet without resorting to ads&#34; could have beneficial consequences for everyone. But I digress...
Registering an account New authors nedd to first register an account, this can be done here. I already had my account set up for my scientific papers.
Registering a blog post  The first informational page on how to register a blog post is given here. It states the following preliminaries have to be met: The blog post has to contain at least 1800 characters (including whitespaces) There my be no copy-restriction on the resulting file (DRM) The text was read by a certain number of people, counted by a VG Wort tracking pixel, or &#34;Zählmarke&#34;  Obtaining the Zählmarken is straightforward: In my VG Wort Account for the &#34;registration of texts online&#34;, or Texte online melden, i.e. T.O.M. in German, I was able to order 100 Zählmarken as a somewhat weirdly formatted csv:
Nevermind the encoding problems... but repeating the header for each line? C&#39;mon!

Keeping Track of the Zählmarken Seeing the poorly formatted csv made me realize I needed to keep track of my Marken in a more sensible way. So I came up with the following (dummy) org-table:
   Filename Zaehlmarke No. Public Key Private Key Post URL No. Characters     vgwort_1.csv 1 &#34;http://vg09.met.vgwort.de/na/foo&#34; bar 2018-01-27-setting-up-a-scalable-rstudio-instance-in-aws.html 12200    Counting the number of characters in all the blog posts using hugo My blog is written in hugo, I&#39;m using the Tranquilpeak theme to be precise. Even though I write most of my articles in org-mode nowadays, there&#39;s still quite a lot of HTML clutter in my source files. How to count the characters excluding such &#34;meta-characters&#34;?
My solution is a bit indirect, but it works quite well. For my RSS feed, I customized the template rss.xml in order to read
... {{ with .Site.Author.email }}&amp;lt;author&amp;gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}&amp;lt;/author&amp;gt;{{end}} &amp;lt;guid&amp;gt;{{ .Permalink }}&amp;lt;/guid&amp;gt; &amp;lt;description&amp;gt;{{ .Content | html }}&amp;lt;/description&amp;gt; ...  The small alteration of using .Content instead of the default .Summary is, that my feed contains &#34;full content&#34;, i.e. the entire post. In order to count the characters in all of my posts, however, I alter the respective line to read
... {{ with .Site.Author.email }}&amp;lt;author&amp;gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}&amp;lt;/author&amp;gt;{{end}} &amp;lt;guid&amp;gt;{{ .Permalink }}&amp;lt;/guid&amp;gt; &amp;lt;description&amp;gt;{{ .Plain | html }}&amp;lt;/description&amp;gt; ...  hugo uses .Plain to wipe away all HTML parts of the content. Hence, I can build my blog locally, open the resulting index.xml in emacs (obviously) and run M-= on the post.
How much money is in it, though? As for this question, I have no answer yet. As of the publishing of this post, I have implemented the Zählmarken in all of the posts, corrected the Datenschutzerklärung (again, obviously) and am now waiting for results. If I understand the process correctly, I will have news by July next year. So, let&#39;s see, eh?
Footnotes  I love the German language. It seems like no word can be considered official enough for law texts unless it has at least 5 syllables. [return]   </description>
    </item>
    
    <item>
      <title>It&#39;s here: Org Agenda for the World Cup 2018</title>
      <link>http://www.sastibe.de/2018/06/its-here-org-agenda-for-the-world-cup-2018/</link>
      <pubDate>Tue, 05 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/06/its-here-org-agenda-for-the-world-cup-2018/</guid>
      <description>As I recently pointed out, I have grown rather fond of Emacs and org-mode especially in recent months. On an entirely unrelated note, the FIFA world cup is right around the corner. Wouldn&#39;t it be nice to combine my passion for the greatest sport in the world (even including all the inevitable diving, arguing with the referees etc...) with my new-found passion for clear and concise org-agendas?
Just so...

A short Google research lead to these two GitHub repositories for the Euro 2012 and the World Cup 2014. As I couldn&#39;t find anything similar for the World Cup 2018, I decided to create it myself.
Here is the result
The content of the schedule is as accurate as I could manage: I read the contents of the raw data into the desired org format via regex (in Emacs, obviously) and manually checked the results against the entries on the wiki page. Writing the regex was actually ... rather fun? A bit weird since it wasn&#39;t pretty, let me show you the first few symbols:
replace-regexp &amp;lt;RET&amp;gt; |\s-+\([0-9]\{1,2\}\) |\([^|]+....  Anyways, it worked, and the result is anyone&#39;s to enjoy. For the record, let me state clearly that I Do Not Intend To Update The Scores In The Repository in any timely fashion. If I get to it, I will, but let&#39;s see.
</description>
    </item>
    
    <item>
      <title>Use Emacs Org Mode and REST APIs for an up-to-date Stock Portfolio</title>
      <link>http://www.sastibe.de/2018/05/2018-05-11-emacs-org-mode-rest-apis-stocks/</link>
      <pubDate>Fri, 11 May 2018 23:13:13 +0200</pubDate>
      
      <guid>http://www.sastibe.de/2018/05/2018-05-11-emacs-org-mode-rest-apis-stocks/</guid>
      <description>A couple of weeks ago, I started to work with Emacs, and I grow fonder of it every day. During a very short time period, it has become my go-to editor for nearly everything I do on my computer, including (but not limited to)
 planning my Todos (in org-mode, to be precise), setting up my agenda (org-mode again), taking memos during meetings writing my (longer) e-mails play around with new stuff write blog posts (this is the first of these...)  It is difficult to pin down exactly why Emacs is taking over so much. My main influences for starting with EMACS were blogposts, the first describing a general EMACS setup, the second detailing how to implement GTD, i.e. Getting Things Done in Emacs org-mode.
In this post, I will demonsrate the strengths of using Emacs in a very specific use case: Getting up-to-date financial data to use in a spread-sheet including all your financial data. Applications like these are usually provided by online banks themselves, so that I don&#39;t show you anything particularly new or shiny. However, the ability to customize every step of the way brings with a number of advantages.
Finding a REST API for Stock Quotes First, we need to get up-to-date financial data from a REST API. I decided to use Alpha Vantage, a site I first stumbled upon after reading this blog post. The other APIs listed on that page had various issues, either being deprecated in the near future (google, yahoo, stooq) or not having a number of symbols (IEX). The API of Alphavantage is rather easy to understand, even though the naming convention is terrible. Try for instance this link using the demo API key, yielding the following result for the Microsoft stock:
{ &amp;quot;Meta Data&amp;quot;: { &amp;quot;1. Information&amp;quot;: &amp;quot;Intraday (1min) prices and volumes&amp;quot;, &amp;quot;2. Symbol&amp;quot;: &amp;quot;MSFT&amp;quot;, &amp;quot;3. Last Refreshed&amp;quot;: &amp;quot;2018-05-11 16:00:00&amp;quot;, &amp;quot;4. Interval&amp;quot;: &amp;quot;1min&amp;quot;, &amp;quot;5. Output Size&amp;quot;: &amp;quot;Compact&amp;quot;, &amp;quot;6. Time Zone&amp;quot;: &amp;quot;US/Eastern&amp;quot; }, &amp;quot;Time Series (1min)&amp;quot;: { &amp;quot;2018-05-11 16:00:00&amp;quot;: { &amp;quot;1. open&amp;quot;: &amp;quot;97.5900&amp;quot;, &amp;quot;2. high&amp;quot;: &amp;quot;97.7300&amp;quot;, &amp;quot;3. low&amp;quot;: &amp;quot;97.5750&amp;quot;, &amp;quot;4. close&amp;quot;: &amp;quot;97.7000&amp;quot;, &amp;quot;5. volume&amp;quot;: &amp;quot;3776187&amp;quot; }, &amp;quot;2018-05-11 15:59:00&amp;quot;: { &amp;quot;1. open&amp;quot;: &amp;quot;97.4800&amp;quot;, &amp;quot;2. high&amp;quot;: &amp;quot;97.5900&amp;quot;, &amp;quot;3. low&amp;quot;: &amp;quot;97.4700&amp;quot;, &amp;quot;4. close&amp;quot;: &amp;quot;97.5900&amp;quot;, &amp;quot;5. volume&amp;quot;: &amp;quot;257615&amp;quot; },...  Reading API Requests into Emacs Variables Having located the data out in the internet was a good first step, but now we need to figure out a way how to use this information. Luckily, most of the work needed for this can be found in various places on the net, for instance in this blog post. I decided to follow their general setup, using the following packages:
 org org-babel request json  The API used in their scenario gave different results with a much cleaner nomenclature. For the Alphavantage API, I had to become a little creative with the eLisp code.
(require &#39;request) (require &#39;json) (require &#39;cl) (request &amp;quot;https://www.alphavantage.co/query&amp;quot; :params `((&amp;quot;function&amp;quot; . &amp;quot;TIME_SERIES_INTRADAY&amp;quot;) (&amp;quot;symbol&amp;quot; . &amp;quot;SC0J&amp;quot;) (&amp;quot;interval&amp;quot; . &amp;quot;1min&amp;quot;) (&amp;quot;apikey&amp;quot; . &amp;quot;...&amp;quot;)) :parser &#39;json-read :success (function* (lambda (&amp;amp;key data &amp;amp;allow-other-keys) (setq open_sc0j (string-to-number (cdr (elt (elt (elt data 1) 1) 1)))))))  The variable open_sc0j is evaluated as follows: From the received json, take the second entry of the second element of the second element. Not very nice, but it works...
I encountered a second difficulty in my portfolio: I have both European stocks (in EUR) and American stocks (traded in USD). In order to keep my balances comparable, I added yet another variable rate_usd_eur, which receives up-to-date exchange rates from USD to EUR from the appropriate query. All in all, my requests to Alphavantage look like this:
(request &amp;quot;https://www.alphavantage.co/query&amp;quot; :params `((&amp;quot;function&amp;quot; . &amp;quot;TIME_SERIES_INTRADAY&amp;quot;) (&amp;quot;symbol&amp;quot; . &amp;quot;SC0J&amp;quot;) (&amp;quot;interval&amp;quot; . &amp;quot;1min&amp;quot;) (&amp;quot;apikey&amp;quot; . &amp;quot;...&amp;quot;)) :parser &#39;json-read :success (function* (lambda (&amp;amp;key data &amp;amp;allow-other-keys) (setq open_sc0j (string-to-number (cdr (elt (elt (elt data 1) 1) 1))))))) (request &amp;quot;https://www.alphavantage.co/query&amp;quot; :params `((&amp;quot;function&amp;quot; . &amp;quot;CURRENCY_EXCHANGE_RATE&amp;quot;) (&amp;quot;from_currency&amp;quot; . &amp;quot;USD&amp;quot;) (&amp;quot;to_currency&amp;quot; . &amp;quot;EUR&amp;quot;) (&amp;quot;apikey&amp;quot; . &amp;quot;...&amp;quot;)) :parser &#39;json-read :success (function* (lambda (&amp;amp;key data &amp;amp;allow-other-keys) (setq rate_usd_eur (string-to-number (cdr (elt (elt data 0) 5))))))) (request &amp;quot;https://www.alphavantage.co/query&amp;quot; :params `((&amp;quot;function&amp;quot; . &amp;quot;TIME_SERIES_INTRADAY&amp;quot;) (&amp;quot;symbol&amp;quot; . &amp;quot;PG&amp;quot;) (&amp;quot;interval&amp;quot; . &amp;quot;1min&amp;quot;) (&amp;quot;apikey&amp;quot; . &amp;quot;...&amp;quot;)) :parser &#39;json-read :success (function* (lambda (&amp;amp;key data &amp;amp;allow-other-keys) (org-table-iterate-buffer-tables) (setq open_prg (* rate_usd_eur (string-to-number (cdr (elt (elt (elt data 1) 1) 1))))))))  Putting this code inside an org-mode file, bracketing it by code blocks #+BEGIN_SRC emacs-lisp :results none and #+END_SRC, and hitting &#34;C-c C-c&#34; inside it leads to the evaluation of the code block and thus the filling of the variables open_sc0j, rate_usd_eur and open_prg. Since we included the wonderful little function org-table-iterate-buffer-tables, the evaluation also repeats until all the columns in the table below are calculated correctly. This neat little trick I also copied from here.
Setting up a Custom Stock Portfolio Org Table After these steps, we now set up an org-table to give us a customizable overview of how our stocks are doing. That means setting up an org-table with columns for historic data, such as the date of the buy. Additionally, we use the #+TBLFM function to calculate appropriate performance indicators. An example for such functions:
| Stock | Symbol | Amt. | Buy | Date Bought | Fees | Dividends | Close | Gain | Gain Perc | Gain per Day | |---------------+--------+------+--------+-----------------+-------+-----------+-------+--------+-----------+--------------| | MSCI ETF | SC0J | 10 | 47.11 | [2018-04-16 Mo] | 12.35 | 0 | | | | | | ProcterGamble | PG | 5 | 65.014 | [2015-10-01 Do] | 10.61 | 72.03 | | | | | #+TBLFM: $9=(-$4 + $8)*$3 - $6 + $7;%0.3f::$10=100*$9/($4*$3)::$11=$9/(now() - $5)::@2$8=&#39;(format &amp;quot;%f&amp;quot; open_sc0j)::@3$7=17.78 + 17.70 + 17.94 + 18.61::@3$8=&#39;(format &amp;quot;%f&amp;quot; open_prg)  This code leads to the following result:
   Stock Symbol Amt. Buy Date Bought Fees Dividends Close Gain Gain Perc Gain per Day     MSCI ETF SC0J 10 47.11 [2018-04-16 Mo] 12.35 0 49.290000 9.450 2.0059435 0.35552367   ProcterGamble PG 5 65.014 [2015-10-01 Do] 10.61 72.03 61.342605 43.063 13.247301 0.045111962    In this example, Gain is first calculated by multiplying -$4 + $8, i.e. the difference between Buy and (today&#39;s) Close by the amount of stocks bought. Additionally, any dividends are added and any fees are subtracted, yielding a &#34;net gain&#34; for the stock. In column Gain per Day, this number is broken down per day since I bought the stock, highlighting my most efficient assets.
There is no limit to what types of functions one can use, and no limit on the sophistication of analysis. And all of this within a very light-weight, easy-to-use interface, without any unnecessary over-head. It is not only convenient, but also educational: while writing this article, I learned a lot about REST APIs and financial data.
Let me conclude this article by picking up a picture from one of the blog posts that got me into Emacs in the first place: Emacs is like a classical steel frame road bike, reliant, robust, nothing fancy but easy to repair. It is the ideal tool to explore the wilderness of the internet. And I can only invite everybody else to come along for the ride.
</description>
    </item>
    
    <item>
      <title>Don&#39;t Worry: Google Only Checks Your Location Every 10 Minutes</title>
      <link>http://www.sastibe.de/2018/04/don-t-worry-google-location/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/04/don-t-worry-google-location/</guid>
      <description>I have a personal Google account, complete with gmail, gdrive and everything else. I first opened it up as a sort of spam email for all kinds of logins, but started to it use more and more due to its convenience. I was always slightly worried about the magnitude of data collected by Google on me, yet I never found a way to pinpoint exactly the extent of my slight worrying.
Recently, I discovered Google Takeout. Everybody with a Google Account can simply click here, follow the instructions and Google Takeout will send all the data it (supposedly) has in a nice little zip folder. Within this zip-folder is a file called “locationhistory.json” (or “standortverlauf.json” for all you German users out there), with entries such as this:
{ &amp;quot;timestampMs&amp;quot; : &amp;quot;1523378268382&amp;quot;, &amp;quot;latitudeE7&amp;quot; : 494290669, &amp;quot;longitudeE7&amp;quot; : 86872541, &amp;quot;accuracy&amp;quot; : 34 } Each of these entries encodes a location measurement taken by Google, with GPS coordinates (latitude/longitude) and a timestamp, which can be converted to a “normal date” by dividing the number by 1000 and using, e.g., this handy site.
The “location history” file is rather large and unwieldy (about 18 MB in my case). There is a very simple and free tool that visualizes your location history data in an interactive heatmap. That is the tool I used to create the intro picture to this entry. The heatmap allows you to gauge the precision with which Google matches your movements. For instance, my skiing trip in March last year to Serfaus-Fiss-Ladis shows up like this:
Don’t worry, I also down some slopes during the vacation…
 There are some mistakes in this map, i.e., places that I have surely never visited. I was never in “Gasthaus zum weißen Lamm”, I know that for a fact. However, the detail is quite astonishing, leading me to the next question: How often does Google measure and store my location data? My “locationhistory.json” contains 59293 observation over the course of 465 days, so that, on average, there are more than 5 measurements per hour.
I decided to look a little closer at the distributions of the timestamps, using some wonderful ggplot magic (the R code can be found here):
Such a colorful mountain range.
 The lines in the plot show the average number of location measurements taken by Google each hour, separated by weekdays. The dashed line indicates the aveerage over all weekdays. The plot highlights several information:
 Between noon and 8 pm, Google takes on average more than one location measurement every 10 minutes In the nighttime, the average number of measurements is only once every 20 minutes Monday and Tuesday mornings are closely watched with many measurements, especially Tuesday mornings Afternoons and evenings are always of interest, but especially on Fridday and Saturday.  The fact that Monday and Tuesday morning are such exceptions could be explained by my specific calendar in 2017: I worked as a consultant and usually left home on Tuesday morning to travel troughout Germany. I am not entirely sure why this should lead to more measurements, as this activity was rarely accompanied by Google services (I travel by Deutsche Bahn). However, my travel time back home, usually late on Thursday evening, can also be seen quite nicely in the plot.
In total, I am a bit shocked by the sheer magnitude of measurements Google has on me, even (and especially) at times at which I am positively certain that I have never used Google Location Services, (see, e.g., 4 am). I am glad that services like Google Maps exist and that they are so extremely convenient, but the drawback should also be made abundantly clear to anyone who uses these services: many machine-readable aspects of your life are available to a for-profit company.
</description>
    </item>
    
    <item>
      <title>Benchmarking AWS Instances with MNIST classification</title>
      <link>http://www.sastibe.de/2018/03/benchmarking-aws-instances/</link>
      <pubDate>Sat, 10 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/03/benchmarking-aws-instances/</guid>
      <description>In a previous post I have shown you how to setup an AWS instance running the newest RStudio, R, Python, Julia and so forth, where the configuration of the instance can be freely chosen. However, there is quite a lot of possibilities of instance configurations out there: There are different instance classes (General Purpose, Compute Optimized, RAM Optimized, … ) and different instance sizes within these classes. For General Purpose, or t2, there are, e.g. t2.nano, t2.micro, t2.small, t2.medium, t2.large, t2.xlarge and t2.2xlarge1.
These instances differ in two dimensions: price and performance. Obviously, these dimensions are highly correlated, since higher price means (or should mean, at least) higher performance. Now, price is easily measured, yet performance is a bit trickier: For example, it is not entirely straightforward to assess the impact of higher RAM, CPU or even GPU directly across many different configurations. But we’re doing data science, right? So why not create a programmatic test in order to gauge the performance empirically? Well, let‘s do it!
The Test For this benchmark test I chose a classical machine learning task: the classification of the MNIST dataset of handwritten digits, to be categorized as 0-9. This data set is very commonly used as an example set for machine learning algorithms.
For this benchmark test, I borrowed a nice skript by Kory Becker written here, which trains a Support Vector Machine (SVM) on the problem, using only the first 1000 observations of the dataset, each with 768 attributes. I altered the code ever so slightly to that each run of the script returns the following measurements:
Elapsed Time: The time elapsed since starting the script (excluding the time to install the libraries and download of the data), Accuracy of model, i.e. the percentage of predictions that classified the digits correctly.  Additionally, I included the following information:
RAM in Gigabytes, Number of CPUs in use, and finally Price in Dollars per Hour.  The Candidates AWS provides a large number of different configurations, and I will not discuss all of these in this post. Rather, let me focus on four different specifications of computing resource demands and chose a distinctive representative:
 General Purpose: t2, m4 Compute Optimized: c4 Memory Optimized: r4  For each of these classes, I had planned to test the sizes small, medium, large, xlarge and 2xlarge. The sizes micro, small and medium are actually only available for t2 (oh, no!), so that I ended up only testing 14 configurations.
 The Results I started with the candidate t2.micro, which is free of charge. Unfortunately, the script never succesfully ran the training of the model, presumably because the dimension of merely 1 GB of RAM is not sufficient. Still, a “not possible” result is still a useful result for choosing the right infrastructure.
Let’s have a first look at the results, first in plain numbers:
  instance_class instance_size ram vcpus ecu price_per_hour elapsed_time accuracy    t2 micro 1.00 1.0 NA 0.0134 NA NA  t2 small 2.00 1.0 NA 0.0268 68.624 0.917  t2 large 8.00 2.0 NA 0.1072 65.335 0.918  t2 xlarge 16.00 4.0 NA 0.2144 55.611 0.918  t2 2xlarge 32.00 8.0 NA 0.4288 56.284 0.919  t2 medium 4.00 2.0 NA 0.0536 63.961 0.919  m4 large 8.00 2.0 6.5 0.1200 82.823 0.933  m4 xlarge 15.00 4.0 13.0 0.2400 80.749 0.928  m4 2xlarge 32.00 8.0 26.0 0.4800 65.728 0.912  m4 4xlarge 64.00 16.0 53.5 0.9600 64.573 0.927  m4 16xlarge 256.00 64.0 188.0 3.8400 93.310 0.915  r4 large 15.25 2.0 7.0 0.1600 80.749 0.928  r4 xlarge 30.50 13.5 4.0 0.3200 68.372 0.920  c4 large 3.75 2.0 8.0 0.1140 121.004 0.915    At a quick glance, the accuracy of the models looks quite uniform. This is hardly surprising, as the algorithm itselg is unchanged by hardware limitation, and the apparent fluctuations can be explained by the stochastic nature of the train-test-data set sampling in the script.
A core assumption is that more computing power yields faster results. A second core assumption is that the higher the computing power, the higher the cost. Combining these assumptions leads us to assume that higher cost leads to a lower time elapsed. A quick visualization of the data demonstrates that the results support this notion: The measurement of the instance “m4.16xlarge” doesn’t quite fit into the pattern, and I am frankly unsure of the reasons. The measurement was taken twice, so that circumstantial errors leading to this measurement can be rejected.
Let us look a little more precisely at the data, in order to establish the most influential factors determining the speed of the analysis. We use the wonderful ggpairs visualization of the GGally package and omit the observation of the instance “m4.16xlarge” in the analysis:
This plot contains a number of results at once. First off, and unsurprisingly, the price per hour correlates vey strongly with the number of virtual CPUs and the size of the RAM, indicating that “the higher the computing power, the higher the cost” was a correct core assumption.
Second, the correlation between Elapsed Eime and the numeric indicators of performance RAM, vCPUs and Price per Hour is clearly negative across the board, but the highest correlation is clearly attained by the dimension RAM. This provides yet another indication that the notion Performance of R hinges on RAM is true.
One last question to consider: which instance type is optimal for R purposes? Optimality will be defined by provide the quickest results for the least money. Compare the fits of a standard linear model:
  instance_class (Intercept) price_per_hour    m4 83.66913 -22.66862  r4 93.12600 -77.35625  t2 66.86029 -29.47335    This shows that the entry price is cheapest for instances of the class “t2”, as the y-intercept is the lowest in this case. However, in cases of higher Price per Hour, i.e. higher necessary computing power, “r4” is the better choice: The time decreases quickest with the increase in power. Both lines meet at a price per hour of roughly 55 Cents per hour, corresponding to an instance r4.2xlarge with 61 GB RAM.
  Takeaway Messages To conclude this article, let me summarize the key findings:
 The most important hardware feature for the increasing computing speed of “R” analysis is RAM For analysis with a small to medium scope of performance (RAM less than 60 GB), the instance class “t2” is the best choice in AWS. For larger scale projects, the instance class “r4”, optimzed for RAM usage, is the optimal choice.  NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration --   …why is it ‚nano‘, ‚micro‘ but then ‚large‘ ‚extra large‘? Be consistent, dangit!↩
   </description>
    </item>
    
    <item>
      <title>My Motivations for Starting a Blog</title>
      <link>http://www.sastibe.de/2018/01/my-motivations-for-starting-a-blog/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/01/my-motivations-for-starting-a-blog/</guid>
      <description>Hello world!
My name is Sebastian Schweer, and I am a Data Scientist. This job description is increasingly popular, but it is notoriously difficult to describe precisely, what that entails. Let me show you one of my favourite definitions:
Source.
 My job requires me to spend a lot of time each day writing code in varying languages, mostly R but also Python and SAS. This inevitably leads me to spend a lot of time thinking about both code as well as the process of programming itself. The major question is, as always, “How do you ensure, that your product is of the best quality?”. Recently, I stumbled upon1 a incredibly concise diagram:
The importance of collaboration
 I believe this is an astute observations, and I find its reflections in many daily situations including (but not limited to) producing code or data analyses. More precisely, I identified these 3 consequences of writing code with the intent of publicizing:
 Tested: Nobody wants to publish content that only works once or only works on a certain local machine. Thus, any project up for publication automatically gets tested and tried much more meticuously. Modular: It is much easier to explain and distribute several single clear ideas than one larger, vague idea. Hence, publication leads to more modular code, creating a more flexible and adaptive code base. Documented: It doesn’t suffice if you as the author understand what the function with non-descriptive names such as fn_011_v3 does, that should be apparent from the name or at least from the documentation. The onus of understanding the code is transferred from the mind of the author to the body of the code.  All of these characteristics increase the maturity and quality of the code. Since I am obviously interested in producing high quality work, I started this blog in order to have a public outlet for all my private little programming projects.
The scope of these projects will vary wildly, I am sure, since the inspiration are heterogeneous. For instance, the first three posts have three different “sponsors”:
 I wrote Setting up an RStudio instance on AWS with the audience of my father in mind, since he showed such an interest in my explanations of the topic over the Christmas break, I wrote (or rather ‘will write’) the post A benchmark for dplyr vs. dbplyr for my sister-in-law, since she asked me about the topic and I didn’t know anything abaout it at the time, and for the present article, or rather statement, I had my former employer in mind, a great fan of simple but concise diagrams depicting deep thoughts.  I appreciate any remarks or comments on anything that I write, and I wish you lots of entertainment perusing my site.
 I can’t find the original source anymore, I have spent a long time going through my Twitter feed. If anyone recognizes the slide and especially the author, I would be incredibly thankful for the information and would gladly update the source information here.↩
   </description>
    </item>
    
    <item>
      <title>Setting up a Scalable RStudio Instance in AWS</title>
      <link>http://www.sastibe.de/2018/01/setting-up-a-scalable-rstudio-instance-in-aws/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/01/setting-up-a-scalable-rstudio-instance-in-aws/</guid>
      <description>Assume you want to start to write R code (a very good decision, in my opinion) and you want to be able to write and test code whereever you are. Wouldn’t it be awesome if one could set up an environment that can be used for R coding independent of any device? Where all you need is a decent browser, a working internet connection and you’re good to go?
Obviously, that is the case. In this post, I will show you the steps for setting up such an environment on Amazon Web Services (AWS). The main advantages of using such a set-up:
 Runs on any infrastructure: All you need is a working internet connection, a decent browser and an AWS account, which is usually1 free. Runs everywhere: The AWS machine will be set up to automatically clone your GitHub repository (don’t worry if this doesn’t mean anything to you, this point is optional), so that you don’t even have to have your codes on the device. Scalable: The AWS machine running your code can be chosen to suit any of your needs, in any session. Just playing around with a new package? Use the smallest size, doesn’t cost a dime. Trying to re-create state-of-the-art machine learning performance with a fancy DNN-classifier? Go all in with 500 GB of RAM; it’ll cost ya, but it’s fun. Up-to-Date: Since the envirionment is freshly installed each time, your R version as well as the package versions in use are automatically up-to-date. In the latter case, that would also be easy to maintain on a local machine, the former, however, is a nice benefit.  Convinced? Awesome, let’s get started!
Overview of main steps First a short overview of the main steps covered in this blog post:
Get an AWS account (duh!), Configure your RStudio AMI, Find the right RStudio AMI, Configure Security Groups, Automatically Change your RStudio Password, Incorporate a clone of your GitHub repo,  Start your First RStudio instance (and bask in its glory), Create a personal AMI for future convenience, Shut down the Instance and all Resources.  Preconditions for this tutorial should be basically none, at least in terms of coding and/or understanding R itself. The main task will lie in clicking the right buttons.
Step 1: Get an AWS account. Well, it isn’t really my place to tell you how to get an AWS account if Amazon itself did such a great job explaining it. Just use the link to set up your account, and I further suggest to follow this set of instructions, building your very first instance. Take your time going through these instructions, I’ll wait…
Ready? Alright, sweet. Then we continue with
 Step 2: Configure your RStudio AMI. In this step, I collected several steps, not all of which are necessary. Steps 2a and 2b are crucial, Step 2c is recommended. Step 2d can be skipped on the first set-up. The implementation of this step can always be re-assessed whenever it becomes necessary.
Let’s begin by starting an instance in the AWS Dashboard. Just open “Instances” on the side menu of your EC2 Dashboard and click on “Launch Instance”:
Here we go!
 Step 2a: Find the current RStudio AMI. The first task is to choose an Amazon Machine Image, or AMI, which is essentially an operating system container. More to the point, in an AMI a Linux distribution can be bundled with addtional software packages tailored to any type of need: web development, accounting (I’m guessing here, but … sure) and, most importantly, using RStudio. On Louis Anslett’s homepage you can find a wonderful storage of RStudio AMIs. We use the newest version for the correct geographical zone, in my case Frankfurt:
One AMI for each region. Neat
 As you can see, thanks to Louis Anslett’s work, the AMI includes not only the newest version of RStudio but also of R itself as well as a handful of helpful additional software packages. For instance, Git comes pre-installed, which we will use later on; also Juliais installed for those looking to try out the possible future of data science languages. But I’m deviating… Let’s note the AMI-ID (in our case “ami-a80db3c7”), put this in the start-up options and let’s continue.
 Step 2b: Configure the security groups for your RStudio instance In AWS, security groups control the access to the machine over the internet (if you don‘t care about how exactly this works and only want to follow the instructions, just skip the next sentences). More precisely, they define which kind of protocols may use which ports on your machine from a given IP range. For example, you can set the access rights for a ssh protocol to be able to connect to your machine on port 22 only from your personal IP address at home.
In our case, we actually only need access via http protocol, since the RStudio instance will allow log-in via browser interface. Therefore, our security group can be kept quite simple:
The bottom option allows the whole world to see the instance. Golly.
 The IP range can be limited to your own personal IP to ensure the safety of your instance. This precaution could be necessary since only the login page of RStudio stands between the internet and your instance (spooky, huh?). However, since the personal IP usually changes each day (roughly speaking), this becomes a personal question of “privacy vs. convenience”. In my case, as you can see, convenience won.
 2c. Automatically Change your RStudio Password In the documentation of the RStudio AMI we can find the following passage: “It is highly recommended you change the password immediately and an easy means of doing this is explained upon login in the script that is loaded there”. Alright, fine, but I’d rather to that programmatically, i.e. automatically. The weirdly named “User data” option provides just the framework: All commands placed here get executed at the beginning of the start-up. You can find this setting in the menu “Configure Instance Details” under “Advanced Details”.
In order to change the password of the user “RStudio” on start-up, we paste the following code:
#!/bin/bash echo &amp;quot;rstudio:guest&amp;quot; | chpasswd where you should replace the password “guest” with whatever you deem appropriate. We are almost done with the set-up now, there only remains
 Step 2d (optional): Automatically Clone a GitHub repo I write all my private code projects on my GitHub account (here: https://github.com/sebastianschweer. What a shameless self-plug!) and I also would like my code to be available for me each time I start up my RStudio instance. Fortunately, this is easily configured with “User data” again, by just adding the command
git clone https://github.com/sebastianschweer/sastibe.git /home/rstudio/sastibe chmod -R 777 /home/rstudio/sastibe to the “User data” of Step 2c. Now, when I start up the new RStudio instance, the repository sastibe gets cloned into the folder /home/rstudio/sastibe, which is automatically loaded in RStudio. The line with chmod ensures that any user (not just root, who is executing this command at startup) has the rights to alter content in that folder. This permission allows me to change code and pushing my changes to the repository and all that, which is just super convenient.
  Step 3: Start your First RStudio instance (and bask in its glory), The last and most exciting click is this one:
Hooray, all the hard work pays off!
 We have now started the instance. This means that a virtual machine, configured according to our specifitcations is being run on one of Amazon’s bajillion2 cloud computing servers. In the menu “Instances” we now see an active instance running. After we are done, we will use this menu to shut it down again (so that it doesn#t cost us), but not now: we are eager to test it out! Accessing the instance is quite easy in our case: Just copy the “IPv4 Public IP” adress and paste it in your browser:
Green lights indicate the instance runs harmoniously.
 Hopefully, you haven’t forgotten your password (check Step 2c if you did), your username is “rstudio”. After succesful login, you’ll be greeted by this screen:
The login to a world of wonder.
 Et voilà: Your very own scalable RStudio instance, accessible world-wide and ready to use at all times. In other words: Congratulations, you now have a state-of-the-art Data Science Machine at your command. Use it wisely. If you want to see what kind of wonders you can do with this setup, check out the upcoming blog post. Otherwise, let me just point you towards another wonderful introduction.
 Step 4: Create a personal AMI for future convenience Now, Step 3 consisted of 4 different steps, and it would be ratehr inconvenient to have to repeat these steps each time you need a new RStudio instance, right? Luckily, AWS has got you covered: You can create an “image” of any AWS instance: simply put, this saves your current configuration for later use. The creation of such an image is straightforward: Just go to “Instances” in your AWS Dashboard, right-click on the machine you want to base the image on and select “Create Image”:
Locate “Create Image” in the menu of “Instance Settings”
 After this step, you will find the created image in the menu AMIs, ready to reuse. Before you go do crazy and wonderful Data Science in your wonderful new Environment, though, it is essential that you let me tell you about
 The Last Step (After Each AWS Usage): Shutting Down An AWS instance doesn’t shut down by itself, or go into hibernation or anything like that. It just keeps running unless otherwise specified, eventually costing lots of money (even the free tier services have their prices after some limit). So, let me show you how to shut down your brand new machine. It’s quite simple, just right-click on the running instance and set the “Instance State” to “Terminate”.
Show no mercy, terminate!
 Since our instance also automatically loaded an EBS volume (like a hard disk to save data), we need to shut that down too. Choose the entry EBS volumes in the sidepane and Detach all volumes that are active. If your overview in the pane “Dashboard” looks similar to this :
“5 volumes”: make sure that they are not in-use, since storage may also cost after some initial period
 There are no hidden services running racking up costs.
  Summary After configuring your AWS environment as decried above, your new ‚Data Science Workflow‘ can look like this:
Log in to AWS, Choose your personal RStudio AMI, Choose the Necessary Specifications of the Machine, Log in to the Machine in the Browser, Do Awesome Data Science, Shut Down Machine and all Resources.  Have fun, and remember: Primere non nocere!
  For a given value of usually. I personally try to test out lots of resources just because I can, yet even so, my total expenses for AWS result in 0.37€ (January 2018).↩
 A rough estimate. Maybe only bajillions.↩
   </description>
    </item>
    
  </channel>
</rss>
