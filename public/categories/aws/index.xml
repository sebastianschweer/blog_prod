<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws on Sastibe&#39;s Data Science Blog</title>
    <link>http://www.sastibe.de/categories/aws/</link>
    <description>Recent content in Aws on Sastibe&#39;s Data Science Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Mar 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.sastibe.de/categories/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Benchmarking AWS Instances with MNIST classification</title>
      <link>http://www.sastibe.de/2018/03/benchmarking-aws-instances/</link>
      <pubDate>Sat, 10 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/03/benchmarking-aws-instances/</guid>
      <description>In a previous post I have shown you how to setup an AWS instance running the newest RStudio, R, Python, Julia and so forth, where the configuration of the instance can be freely chosen. However, there is quite a lot of possibilities of instance configurations out there: There are different instance classes (General Purpose, Compute Optimized, RAM Optimized, … ) and different instance sizes within these classes. For General Purpose, or t2, there are, e.g. t2.nano, t2.micro, t2.small, t2.medium, t2.large, t2.xlarge and t2.2xlarge1.
These instances differ in two dimensions: price and performance. Obviously, these dimensions are highly correlated, since higher price means (or should mean, at least) higher performance. Now, price is easily measured, yet performance is a bit trickier: For example, it is not entirely straightforward to assess the impact of higher RAM, CPU or even GPU directly across many different configurations. But we’re doing data science, right? So why not create a programmatic test in order to gauge the performance empirically? Well, let‘s do it!
The Test For this benchmark test I chose a classical machine learning task: the classification of the MNIST dataset of handwritten digits, to be categorized as 0-9. This data set is very commonly used as an example set for machine learning algorithms.
For this benchmark test, I borrowed a nice skript by Kory Becker written here, which trains a Support Vector Machine (SVM) on the problem, using only the first 1000 observations of the dataset, each with 768 attributes. I altered the code ever so slightly to that each run of the script returns the following measurements:
Elapsed Time: The time elapsed since starting the script (excluding the time to install the libraries and download of the data), Accuracy of model, i.e. the percentage of predictions that classified the digits correctly.  Additionally, I included the following information:
RAM in Gigabytes, Number of CPUs in use, and finally Price in Dollars per Hour.  The Candidates AWS provides a large number of different configurations, and I will not discuss all of these in this post. Rather, let me focus on four different specifications of computing resource demands and chose a distinctive representative:
 General Purpose: t2, m4 Compute Optimized: c4 Memory Optimized: r4  For each of these classes, I had planned to test the sizes small, medium, large, xlarge and 2xlarge. The sizes micro, small and medium are actually only available for t2 (oh, no!), so that I ended up only testing 14 configurations.
 The Results I started with the candidate t2.micro, which is free of charge. Unfortunately, the script never succesfully ran the training of the model, presumably because the dimension of merely 1 GB of RAM is not sufficient. Still, a “not possible” result is still a useful result for choosing the right infrastructure.
Let’s have a first look at the results, first in plain numbers:
  instance_class instance_size ram vcpus ecu price_per_hour elapsed_time accuracy    t2 micro 1.00 1.0 NA 0.0134 NA NA  t2 small 2.00 1.0 NA 0.0268 68.624 0.917  t2 large 8.00 2.0 NA 0.1072 65.335 0.918  t2 xlarge 16.00 4.0 NA 0.2144 55.611 0.918  t2 2xlarge 32.00 8.0 NA 0.4288 56.284 0.919  t2 medium 4.00 2.0 NA 0.0536 63.961 0.919  m4 large 8.00 2.0 6.5 0.1200 82.823 0.933  m4 xlarge 15.00 4.0 13.0 0.2400 80.749 0.928  m4 2xlarge 32.00 8.0 26.0 0.4800 65.728 0.912  m4 4xlarge 64.00 16.0 53.5 0.9600 64.573 0.927  m4 16xlarge 256.00 64.0 188.0 3.8400 93.310 0.915  r4 large 15.25 2.0 7.0 0.1600 80.749 0.928  r4 xlarge 30.50 13.5 4.0 0.3200 68.372 0.920  c4 large 3.75 2.0 8.0 0.1140 121.004 0.915    At a quick glance, the accuracy of the models looks quite uniform. This is hardly surprising, as the algorithm itselg is unchanged by hardware limitation, and the apparent fluctuations can be explained by the stochastic nature of the train-test-data set sampling in the script.
A core assumption is that more computing power yields faster results. A second core assumption is that the higher the computing power, the higher the cost. Combining these assumptions leads us to assume that higher cost leads to a lower time elapsed. A quick visualization of the data demonstrates that the results support this notion: The measurement of the instance “m4.16xlarge” doesn’t quite fit into the pattern, and I am frankly unsure of the reasons. The measurement was taken twice, so that circumstantial errors leading to this measurement can be rejected.
Let us look a little more precisely at the data, in order to establish the most influential factors determining the speed of the analysis. We use the wonderful ggpairs visualization of the GGally package and omit the observation of the instance “m4.16xlarge” in the analysis:
This plot contains a number of results at once. First off, and unsurprisingly, the price per hour correlates vey strongly with the number of virtual CPUs and the size of the RAM, indicating that “the higher the computing power, the higher the cost” was a correct core assumption.
Second, the correlation between Elapsed Eime and the numeric indicators of performance RAM, vCPUs and Price per Hour is clearly negative across the board, but the highest correlation is clearly attained by the dimension RAM. This provides yet another indication that the notion Performance of R hinges on RAM is true.
One last question to consider: which instance type is optimal for R purposes? Optimality will be defined by provide the quickest results for the least money. Compare the fits of a standard linear model:
  instance_class (Intercept) price_per_hour    m4 83.66913 -22.66862  r4 93.12600 -77.35625  t2 66.86029 -29.47335    This shows that the entry price is cheapest for instances of the class “t2”, as the y-intercept is the lowest in this case. However, in cases of higher Price per Hour, i.e. higher necessary computing power, “r4” is the better choice: The time decreases quickest with the increase in power. Both lines meet at a price per hour of roughly 55 Cents per hour, corresponding to an instance r4.2xlarge with 61 GB RAM.
  Takeaway Messages To conclude this article, let me summarize the key findings:
 The most important hardware feature for the increasing computing speed of “R” analysis is RAM For analysis with a small to medium scope of performance (RAM less than 60 GB), the instance class “t2” is the best choice in AWS. For larger scale projects, the instance class “r4”, optimzed for RAM usage, is the optimal choice.  NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration -- NIR] : duration duration --   …why is it ‚nano‘, ‚micro‘ but then ‚large‘ ‚extra large‘? Be consistent, dangit!↩
   </description>
    </item>
    
    <item>
      <title>Setting up a Scalable RStudio Instance in AWS</title>
      <link>http://www.sastibe.de/2018/01/setting-up-a-scalable-rstudio-instance-in-aws/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/01/setting-up-a-scalable-rstudio-instance-in-aws/</guid>
      <description>Assume you want to start to write R code (a very good decision, in my opinion) and you want to be able to write and test code whereever you are. Wouldn’t it be awesome if one could set up an environment that can be used for R coding independent of any device? Where all you need is a decent browser, a working internet connection and you’re good to go?
Obviously, that is the case. In this post, I will show you the steps for setting up such an environment on Amazon Web Services (AWS). The main advantages of using such a set-up:
 Runs on any infrastructure: All you need is a working internet connection, a decent browser and an AWS account, which is usually1 free. Runs everywhere: The AWS machine will be set up to automatically clone your GitHub repository (don’t worry if this doesn’t mean anything to you, this point is optional), so that you don’t even have to have your codes on the device. Scalable: The AWS machine running your code can be chosen to suit any of your needs, in any session. Just playing around with a new package? Use the smallest size, doesn’t cost a dime. Trying to re-create state-of-the-art machine learning performance with a fancy DNN-classifier? Go all in with 500 GB of RAM; it’ll cost ya, but it’s fun. Up-to-Date: Since the envirionment is freshly installed each time, your R version as well as the package versions in use are automatically up-to-date. In the latter case, that would also be easy to maintain on a local machine, the former, however, is a nice benefit.  Convinced? Awesome, let’s get started!
Overview of main steps First a short overview of the main steps covered in this blog post:
Get an AWS account (duh!), Configure your RStudio AMI, Find the right RStudio AMI, Configure Security Groups, Automatically Change your RStudio Password, Incorporate a clone of your GitHub repo,  Start your First RStudio instance (and bask in its glory), Create a personal AMI for future convenience, Shut down the Instance and all Resources.  Preconditions for this tutorial should be basically none, at least in terms of coding and/or understanding R itself. The main task will lie in clicking the right buttons.
Step 1: Get an AWS account. Well, it isn’t really my place to tell you how to get an AWS account if Amazon itself did such a great job explaining it. Just use the link to set up your account, and I further suggest to follow this set of instructions, building your very first instance. Take your time going through these instructions, I’ll wait…
Ready? Alright, sweet. Then we continue with
 Step 2: Configure your RStudio AMI. In this step, I collected several steps, not all of which are necessary. Steps 2a and 2b are crucial, Step 2c is recommended. Step 2d can be skipped on the first set-up. The implementation of this step can always be re-assessed whenever it becomes necessary.
Let’s begin by starting an instance in the AWS Dashboard. Just open “Instances” on the side menu of your EC2 Dashboard and click on “Launch Instance”:
Here we go!
 Step 2a: Find the current RStudio AMI. The first task is to choose an Amazon Machine Image, or AMI, which is essentially an operating system container. More to the point, in an AMI a Linux distribution can be bundled with addtional software packages tailored to any type of need: web development, accounting (I’m guessing here, but … sure) and, most importantly, using RStudio. On Louis Anslett’s homepage you can find a wonderful storage of RStudio AMIs. We use the newest version for the correct geographical zone, in my case Frankfurt:
One AMI for each region. Neat
 As you can see, thanks to Louis Anslett’s work, the AMI includes not only the newest version of RStudio but also of R itself as well as a handful of helpful additional software packages. For instance, Git comes pre-installed, which we will use later on; also Juliais installed for those looking to try out the possible future of data science languages. But I’m deviating… Let’s note the AMI-ID (in our case “ami-a80db3c7”), put this in the start-up options and let’s continue.
 Step 2b: Configure the security groups for your RStudio instance In AWS, security groups control the access to the machine over the internet (if you don‘t care about how exactly this works and only want to follow the instructions, just skip the next sentences). More precisely, they define which kind of protocols may use which ports on your machine from a given IP range. For example, you can set the access rights for a ssh protocol to be able to connect to your machine on port 22 only from your personal IP address at home.
In our case, we actually only need access via http protocol, since the RStudio instance will allow log-in via browser interface. Therefore, our security group can be kept quite simple:
The bottom option allows the whole world to see the instance. Golly.
 The IP range can be limited to your own personal IP to ensure the safety of your instance. This precaution could be necessary since only the login page of RStudio stands between the internet and your instance (spooky, huh?). However, since the personal IP usually changes each day (roughly speaking), this becomes a personal question of “privacy vs. convenience”. In my case, as you can see, convenience won.
 2c. Automatically Change your RStudio Password In the documentation of the RStudio AMI we can find the following passage: “It is highly recommended you change the password immediately and an easy means of doing this is explained upon login in the script that is loaded there”. Alright, fine, but I’d rather to that programmatically, i.e. automatically. The weirdly named “User data” option provides just the framework: All commands placed here get executed at the beginning of the start-up. You can find this setting in the menu “Configure Instance Details” under “Advanced Details”.
In order to change the password of the user “RStudio” on start-up, we paste the following code:
#!/bin/bash echo &amp;quot;rstudio:guest&amp;quot; | chpasswd where you should replace the password “guest” with whatever you deem appropriate. We are almost done with the set-up now, there only remains
 Step 2d (optional): Automatically Clone a GitHub repo I write all my private code projects on my GitHub account (here: https://github.com/sebastianschweer. What a shameless self-plug!) and I also would like my code to be available for me each time I start up my RStudio instance. Fortunately, this is easily configured with “User data” again, by just adding the command
git clone https://github.com/sebastianschweer/sastibe.git /home/rstudio/sastibe chmod -R 777 /home/rstudio/sastibe to the “User data” of Step 2c. Now, when I start up the new RStudio instance, the repository sastibe gets cloned into the folder /home/rstudio/sastibe, which is automatically loaded in RStudio. The line with chmod ensures that any user (not just root, who is executing this command at startup) has the rights to alter content in that folder. This permission allows me to change code and pushing my changes to the repository and all that, which is just super convenient.
  Step 3: Start your First RStudio instance (and bask in its glory), The last and most exciting click is this one:
Hooray, all the hard work pays off!
 We have now started the instance. This means that a virtual machine, configured according to our specifitcations is being run on one of Amazon’s bajillion2 cloud computing servers. In the menu “Instances” we now see an active instance running. After we are done, we will use this menu to shut it down again (so that it doesn#t cost us), but not now: we are eager to test it out! Accessing the instance is quite easy in our case: Just copy the “IPv4 Public IP” adress and paste it in your browser:
Green lights indicate the instance runs harmoniously.
 Hopefully, you haven’t forgotten your password (check Step 2c if you did), your username is “rstudio”. After succesful login, you’ll be greeted by this screen:
The login to a world of wonder.
 Et voilà: Your very own scalable RStudio instance, accessible world-wide and ready to use at all times. In other words: Congratulations, you now have a state-of-the-art Data Science Machine at your command. Use it wisely. If you want to see what kind of wonders you can do with this setup, check out the upcoming blog post. Otherwise, let me just point you towards another wonderful introduction.
 Step 4: Create a personal AMI for future convenience Now, Step 3 consisted of 4 different steps, and it would be ratehr inconvenient to have to repeat these steps each time you need a new RStudio instance, right? Luckily, AWS has got you covered: You can create an “image” of any AWS instance: simply put, this saves your current configuration for later use. The creation of such an image is straightforward: Just go to “Instances” in your AWS Dashboard, right-click on the machine you want to base the image on and select “Create Image”:
Locate “Create Image” in the menu of “Instance Settings”
 After this step, you will find the created image in the menu AMIs, ready to reuse. Before you go do crazy and wonderful Data Science in your wonderful new Environment, though, it is essential that you let me tell you about
 The Last Step (After Each AWS Usage): Shutting Down An AWS instance doesn’t shut down by itself, or go into hibernation or anything like that. It just keeps running unless otherwise specified, eventually costing lots of money (even the free tier services have their prices after some limit). So, let me show you how to shut down your brand new machine. It’s quite simple, just right-click on the running instance and set the “Instance State” to “Terminate”.
Show no mercy, terminate!
 Since our instance also automatically loaded an EBS volume (like a hard disk to save data), we need to shut that down too. Choose the entry EBS volumes in the sidepane and Detach all volumes that are active. If your overview in the pane “Dashboard” looks similar to this :
“5 volumes”: make sure that they are not in-use, since storage may also cost after some initial period
 There are no hidden services running racking up costs.
  Summary After configuring your AWS environment as decried above, your new ‚Data Science Workflow‘ can look like this:
Log in to AWS, Choose your personal RStudio AMI, Choose the Necessary Specifications of the Machine, Log in to the Machine in the Browser, Do Awesome Data Science, Shut Down Machine and all Resources.  Have fun, and remember: Primere non nocere!
  For a given value of usually. I personally try to test out lots of resources just because I can, yet even so, my total expenses for AWS result in 0.37€ (January 2018).↩
 A rough estimate. Maybe only bajillions.↩
   </description>
    </item>
    
  </channel>
</rss>
