<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on Sastibe&#39;s Data Science Blog</title>
    <link>http://www.sastibe.de/tags/programming/</link>
    <description>Recent content in Programming on Sastibe&#39;s Data Science Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Apr 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://www.sastibe.de/tags/programming/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Don&#39;t Worry: Google Only Checks Your Location Every 10 Minutes</title>
      <link>http://www.sastibe.de/2018/04/don-t-worry-google-location/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/04/don-t-worry-google-location/</guid>
      <description>I have a personal Google account, complete with gmail, gdrive and everything else. I first opened it up as a sort of spam email for all kinds of logins, but started to it use more and more due to its convenience. I was always slightly worried about the magnitude of data collected by Google on me, yet I never found a way to pinpoint exactly the extent of my slight worrying.
Recently, I discovered Google Takeout. Everybody with a Google Account can simply click here, follow the instructions and Google Takeout will send all the data it (supposedly) has in a nice little zip folder. Within this zip-folder is a file called “locationhistory.json” (or “standortverlauf.json” for all you German users out there), with entries such as this:
{ &amp;quot;timestampMs&amp;quot; : &amp;quot;1523378268382&amp;quot;, &amp;quot;latitudeE7&amp;quot; : 494290669, &amp;quot;longitudeE7&amp;quot; : 86872541, &amp;quot;accuracy&amp;quot; : 34 } Each of these entries encodes a location measurement taken by Google, with GPS coordinates (latitude/longitude) and a timestamp, which can be converted to a “normal date” by dividing the number by 1000 and using, e.g., this handy site.
The “location history” file is rather large and unwieldy (about 18 MB in my case). There is a very simple and free tool that visualizes your location history data in an interactive heatmap. That is the tool I used to create the intro picture to this entry. The heatmap allows you to gauge the precision with which Google matches your movements. For instance, my skiing trip in March last year to Serfaus-Fiss-Ladis shows up like this:
Don’t worry, I also down some slopes during the vacation…
 There are some mistakes in this map, i.e., places that I have surely never visited. I was never in “Gasthaus zum weißen Lamm”, I know that for a fact. However, the detail is quite astonishing, leading me to the next question: How often does Google measure and store my location data? My “locationhistory.json” contains 59293 observation over the course of 465 days, so that, on average, there are more than 5 measurements per hour.
I decided to look a little closer at the distributions of the timestamps, using some wonderful ggplot magic (the R code can be found here):
Such a colorful mountain range.
 The lines in the plot show the average number of location measurements taken by Google each hour, separated by weekdays. The dashed line indicates the aveerage over all weekdays. The plot highlights several information:
 Between noon and 8 pm, Google takes on average more than one location measurement every 10 minutes In the nighttime, the average number of measurements is only once every 20 minutes Monday and Tuesday mornings are closely watched with many measurements, especially Tuesday mornings Afternoons and evenings are always of interest, but especially on Fridday and Saturday.  The fact that Monday and Tuesday morning are such exceptions could be explained by my specific calendar in 2017: I worked as a consultant and usually left home on Tuesday morning to travel troughout Germany. I am not entirely sure why this should lead to more measurements, as this activity was rarely accompanied by Google services (I travel by Deutsche Bahn). However, my travel time back home, usually late on Thursday evening, can also be seen quite nicely in the plot.
In total, I am a bit shocked by the sheer magnitude of measurements Google has on me, even (and especially) at times at which I am positively certain that I have never used Google Location Services, (see, e.g., 4 am). I am glad that services like Google Maps exist and that they are so extremely convenient, but the drawback should also be made abundantly clear to anyone who uses these services: many machine-readable aspects of your life are available to a for-profit company.
</description>
    </item>
    
    <item>
      <title>My Motivations for Starting a Blog</title>
      <link>http://www.sastibe.de/2018/01/my-motivations-for-starting-a-blog/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.sastibe.de/2018/01/my-motivations-for-starting-a-blog/</guid>
      <description>Hello world!
My name is Sebastian Schweer, and I am a Data Scientist. This job description is increasingly popular, but it is notoriously difficult to describe precisely, what that entails. Let me show you one of my favourite definitions:
Source.
 My job requires me to spend a lot of time each day writing code in varying languages, mostly R but also Python and SAS. This inevitably leads me to spend a lot of time thinking about both code as well as the process of programming itself. The major question is, as always, “How do you ensure, that your product is of the best quality?”. Recently, I stumbled upon1 a incredibly concise diagram:
The importance of collaboration
 I believe this is an astute observations, and I find its reflections in many daily situations including (but not limited to) producing code or data analyses. More precisely, I identified these 3 consequences of writing code with the intent of publicizing:
 Tested: Nobody wants to publish content that only works once or only works on a certain local machine. Thus, any project up for publication automatically gets tested and tried much more meticuously. Modular: It is much easier to explain and distribute several single clear ideas than one larger, vague idea. Hence, publication leads to more modular code, creating a more flexible and adaptive code base. Documented: It doesn’t suffice if you as the author understand what the function with non-descriptive names such as fn_011_v3 does, that should be apparent from the name or at least from the documentation. The onus of understanding the code is transferred from the mind of the author to the body of the code.  All of these characteristics increase the maturity and quality of the code. Since I am obviously interested in producing high quality work, I started this blog in order to have a public outlet for all my private little programming projects.
The scope of these projects will vary wildly, I am sure, since the inspiration are heterogeneous. For instance, the first three posts have three different “sponsors”:
 I wrote Setting up an RStudio instance on AWS with the audience of my father in mind, since he showed such an interest in my explanations of the topic over the Christmas break, I wrote (or rather ‘will write’) the post A benchmark for dplyr vs. dbplyr for my sister-in-law, since she asked me about the topic and I didn’t know anything abaout it at the time, and for the present article, or rather statement, I had my former employer in mind, a great fan of simple but concise diagrams depicting deep thoughts.  I appreciate any remarks or comments on anything that I write, and I wish you lots of entertainment perusing my site.
 I can’t find the original source anymore, I have spent a long time going through my Twitter feed. If anyone recognizes the slide and especially the author, I would be incredibly thankful for the information and would gladly update the source information here.↩
   </description>
    </item>
    
  </channel>
</rss>
